{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e11e4c4",
   "metadata": {},
   "source": [
    "# CT Reconstruction Pipeline Notebook\n",
    "\n",
    "This notebook guides through the process of reconstructing 3D volume data from 2D projection images using the modules developed in the `src` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1c4393",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import necessary libraries and the custom modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc8ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the src directory is in the Python path\n",
    "# If running the notebook from the project root directory, this should work:\n",
    "import sys\n",
    "if './src' not in sys.path:\n",
    "    sys.path.append('./src')\n",
    "\n",
    "# Import custom modules\n",
    "import config as cfg\n",
    "import utils\n",
    "import data_io\n",
    "import preprocess\n",
    "import reconstruction\n",
    "import visualization\n",
    "\n",
    "# Configure matplotlib for inline display in Jupyter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c415d",
   "metadata": {},
   "source": [
    "## 2. Configuration Loading\n",
    "\n",
    "Load parameters from the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5b1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:34:54,201 - config - INFO - Successfully loaded configuration from: config/default_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully:\n",
      "{\n",
      "  \"data_base_dir\": \"./data/\",\n",
      "  \"dataset_name\": \"dataset_1\",\n",
      "  \"preprocess_epsilon\": 1e-06,\n",
      "  \"reconstruction_algorithm\": \"fbp\",\n",
      "  \"fbp_filter\": \"ramp\",\n",
      "  \"output_slice_size\": null,\n",
      "  \"results_base_dir\": \"./results/\",\n",
      "  \"output_prefix\": \"recon\",\n",
      "  \"log_level\": \"INFO\",\n",
      "  \"log_file\": \"pipeline.log\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the configuration file\n",
    "CONFIG_FILE = Path(\"./config/default_params.yaml\")\n",
    "\n",
    "# Load configuration\n",
    "config = cfg.load_config(CONFIG_FILE)\n",
    "\n",
    "if config:\n",
    "    print(\"Configuration loaded successfully:\")\n",
    "    # Pretty print the config dictionary\n",
    "    import json\n",
    "    print(json.dumps(config, indent=2))\n",
    "else:\n",
    "    raise RuntimeError(f\"Failed to load configuration from {CONFIG_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5d1bb",
   "metadata": {},
   "source": [
    "## 3. Logging Setup\n",
    "\n",
    "Configure logging based on settings from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aaac76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:34:54 - root - INFO - Logging configured. Level: INFO. Outputting to console and file: results/pipeline.log\n",
      "2025-05-03 14:34:54 - root - INFO - Logging initialized for the notebook.\n"
     ]
    }
   ],
   "source": [
    "# Get logging parameters from config\n",
    "log_level_str = config.get('log_level', 'INFO').upper()\n",
    "log_level = getattr(logging, log_level_str, logging.INFO)\n",
    "log_file_config = config.get('log_file', None) # Can be null/None in YAML\n",
    "\n",
    "# Construct full log file path if specified\n",
    "log_file_path = None\n",
    "if log_file_config:\n",
    "    # Assume log file path is relative to results base dir or project root\n",
    "    # Here, let's put it in the results base directory\n",
    "    results_base = Path(config.get('results_base_dir', './results/'))\n",
    "    log_file_path = results_base / log_file_config\n",
    "\n",
    "# Setup logging using the utility function\n",
    "utils.setup_logging(level=log_level, log_file=log_file_path)\n",
    "\n",
    "logging.info(\"Logging initialized for the notebook.\")\n",
    "logging.debug(f\"Using configuration: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f293a66",
   "metadata": {},
   "source": [
    "## 4. Define Data Paths\n",
    "\n",
    "Construct the specific paths for the dataset being processed based on the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836741ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:34:54 - root - INFO - Processing dataset: dataset_1\n",
      "2025-05-03 14:34:54 - root - INFO - Projections directory: data/dataset_1/projections\n",
      "2025-05-03 14:34:54 - root - INFO - Flats directory: data/dataset_1/flats\n",
      "2025-05-03 14:34:54 - root - INFO - Darks directory: data/dataset_1/darks\n",
      "2025-05-03 14:34:54 - root - INFO - Metadata file: data/dataset_1/metadata.txt\n"
     ]
    }
   ],
   "source": [
    "data_base_dir = Path(config.get('data_base_dir', './data/'))\n",
    "dataset_name = config.get('dataset_name', 'default_dataset')\n",
    "dataset_dir = data_base_dir / dataset_name\n",
    "\n",
    "proj_dir = dataset_dir / \"projections\"\n",
    "flat_dir = dataset_dir / \"flats\"\n",
    "dark_dir = dataset_dir / \"darks\"\n",
    "metadata_file = dataset_dir / \"metadata.txt\" # Or whatever the metadata file is named\n",
    "\n",
    "logging.info(f\"Processing dataset: {dataset_name}\")\n",
    "logging.info(f\"Projections directory: {proj_dir}\")\n",
    "logging.info(f\"Flats directory: {flat_dir}\")\n",
    "logging.info(f\"Darks directory: {dark_dir}\")\n",
    "logging.info(f\"Metadata file: {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ed389",
   "metadata": {},
   "source": [
    "## 5. Data Loading\n",
    "\n",
    "Load the projection images, flat fields, dark fields, and metadata (angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f882c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:34:58 - root - INFO - Found 361 TIFF files in data/dataset_1/projections. Loading sequence...\n",
      "2025-05-03 14:35:08 - root - INFO - Successfully loaded image stack with shape: (361, 2368, 2240)\n",
      "2025-05-03 14:35:08 - root - ERROR - Error: Provided path 'data/dataset_1/flats' is not a valid directory.\n",
      "2025-05-03 14:35:08 - root - ERROR - Error: Provided path 'data/dataset_1/darks' is not a valid directory.\n",
      "2025-05-03 14:35:08 - root - INFO - Read from metadata: NumberImages=360, AngleFirst=0.0, AngleInterval=1.0\n",
      "2025-05-03 14:35:08 - root - INFO - Successfully calculated 360 angles (radians) from metadata.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load one or more data components. Check logs and paths.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Basic checks\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m projections \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m flats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m darks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m angles_rad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFailed to load one or more data components. Check logs and paths.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded projections shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprojections.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded flats shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflats.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Failed to load one or more data components. Check logs and paths."
     ]
    }
   ],
   "source": [
    "# Load data using data_io module\n",
    "projections = data_io.load_tiff_sequence(proj_dir)\n",
    "flats = data_io.load_tiff_sequence(flat_dir)\n",
    "darks = data_io.load_tiff_sequence(dark_dir)\n",
    "angles_rad = data_io.load_metadata(metadata_file)\n",
    "\n",
    "# Basic checks\n",
    "if projections is None or flats is None or darks is None or angles_rad is None:\n",
    "    raise ValueError(\"Failed to load one or more data components. Check logs and paths.\")\n",
    "\n",
    "logging.info(f\"Loaded projections shape: {projections.shape}\")\n",
    "logging.info(f\"Loaded flats shape: {flats.shape}\")\n",
    "logging.info(f\"Loaded darks shape: {darks.shape}\")\n",
    "logging.info(f\"Loaded angles shape: {angles_rad.shape}\")\n",
    "\n",
    "# Verify consistency\n",
    "if projections.shape[0] != len(angles_rad):\n",
    "    raise ValueError(\"Number of projections does not match number of angles!\")\n",
    "if projections.shape[1:] != flats.shape[1:] or projections.shape[1:] != darks.shape[1:]:\n",
    "    raise ValueError(\"Image dimensions (height, width) do not match between projections, flats, and darks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b28423",
   "metadata": {},
   "source": [
    "## 6. Data Exploration (Optional)\n",
    "\n",
    "Visualize some of the raw loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ffe59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside Cell 6\n",
    "avg_flat = preprocess.average_images(flats) if flats is not None else None\n",
    "avg_dark = preprocess.average_images(darks) if darks is not None else None\n",
    "\n",
    "if avg_flat is not None and avg_dark is not None:\n",
    "    visualization.plot_comparison(avg_flat, avg_dark, title1=\"Average Flat Field\", title2=\"Average Dark Field\", main_title=\"Calibration Frames\", colorbar_label=\"Counts\")\n",
    "    plt.show()\n",
    "else:\n",
    "    logging.warning(\"Average flats/darks cannot be visualized as one or both were not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec31fc3",
   "metadata": {},
   "source": [
    "## 7. Preprocessing\n",
    "\n",
    "Apply dark subtraction, flat-field correction, and negative logarithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get epsilon from config\n",
    "epsilon = config.get('preprocess_epsilon', 1e-6)\n",
    "\n",
    "# Perform preprocessing\n",
    "attenuation_sinograms = preprocess.preprocess_data(projections, flats, darks, epsilon=epsilon)\n",
    "\n",
    "if attenuation_sinograms is None:\n",
    "    raise RuntimeError(\"Preprocessing failed. Check logs.\")\n",
    "\n",
    "logging.info(f\"Preprocessing complete. Attenuation data shape: {attenuation_sinograms.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85105da0",
   "metadata": {},
   "source": [
    "## 8. Preprocessing Visualization (Optional)\n",
    "\n",
    "Visualize a sinogram slice after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd0bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside Cell 8\n",
    "if attenuation_sinograms is not None:\n",
    "    height = attenuation_sinograms.shape[1]\n",
    "    middle_slice_idx = height // 2\n",
    "    sinogram_slice = attenuation_sinograms[:, middle_slice_idx, :]\n",
    "    # Check if correction was likely skipped (by comparing with raw projections)\n",
    "    # A more robust check might involve a flag returned from preprocess_data\n",
    "    was_corrected = not np.allclose(attenuation_sinograms, projections.astype(np.float32))\n",
    "\n",
    "    plot_title = f\"Sinogram (Slice {middle_slice_idx})\"\n",
    "    colorbar_lbl = \"Value\"\n",
    "    if was_corrected:\n",
    "        plot_title = f\"Preprocessed Sinogram (Slice {middle_slice_idx})\"\n",
    "        colorbar_lbl = \"Attenuation\"\n",
    "    else:\n",
    "         plot_title = f\"Raw Sinogram (Slice {middle_slice_idx} - No Correction)\"\n",
    "         colorbar_lbl = \"Counts / Intensity\"\n",
    "\n",
    "    visualization.plot_sinogram(sinogram_slice, title=plot_title, colorbar_label=colorbar_lbl)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26b0af",
   "metadata": {},
   "source": [
    "## 9. Reconstruction\n",
    "\n",
    "Perform tomographic reconstruction using the chosen algorithm (e.g., FBP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43178bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction parameters from config\n",
    "algorithm = config.get('reconstruction_algorithm', 'fbp').lower()\n",
    "fbp_filter = config.get('fbp_filter', 'ramp')\n",
    "output_size = config.get('output_slice_size', None) # Can be null/None\n",
    "\n",
    "reconstructed_volume = None\n",
    "if algorithm == 'fbp':\n",
    "    logging.info(f\"Performing FBP reconstruction with filter: {fbp_filter}\")\n",
    "    reconstructed_volume = reconstruction.reconstruct_fbp(\n",
    "        attenuation_sinograms,\n",
    "        angles_rad,\n",
    "        filter_name=fbp_filter,\n",
    "        output_size=output_size\n",
    "    )\n",
    "elif algorithm == 'sirt':\n",
    "    # Placeholder for future SIRT implementation\n",
    "    # sirt_iterations = config.get('sirt_iterations', 100)\n",
    "    logging.warning(\"SIRT reconstruction not yet implemented in reconstruction.py module.\")\n",
    "    # reconstructed_volume = reconstruction.reconstruct_sirt(attenuation_sinograms, angles_rad, iterations=sirt_iterations)\n",
    "else:\n",
    "    logging.error(f\"Unsupported reconstruction algorithm specified: {algorithm}\")\n",
    "\n",
    "if reconstructed_volume is None and algorithm == 'fbp': # Check if FBP was attempted but failed\n",
    "     raise RuntimeError(\"Reconstruction failed. Check logs.\")\n",
    "elif reconstructed_volume is not None:\n",
    "    logging.info(f\"Reconstruction complete. Volume shape: {reconstructed_volume.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29791b66",
   "metadata": {},
   "source": [
    "## 10. Result Visualization\n",
    "\n",
    "Display some slices from the reconstructed 3D volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reconstructed_volume is not None:\n",
    "    num_recon_slices = reconstructed_volume.shape[0]\n",
    "\n",
    "    # Show middle slice\n",
    "    middle_recon_slice_idx = num_recon_slices // 2\n",
    "    visualization.plot_slice(reconstructed_volume[middle_recon_slice_idx],\n",
    "                             title=f\"Reconstructed Slice {middle_recon_slice_idx} ({algorithm.upper()})\",\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde15c6d",
   "metadata": {},
   "source": [
    "## 11. Saving Results\n",
    "\n",
    "Save the reconstructed volume as a stack of TIFF slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reconstructed_volume is not None:\n",
    "    # Define output directory and prefix\n",
    "    results_base_dir = Path(config.get('results_base_dir', './results/'))\n",
    "    output_dir_name = f\"{dataset_name}_recon_{algorithm}\" # e.g., dataset_1_recon_fbp\n",
    "    output_directory = results_base_dir / output_dir_name\n",
    "    output_prefix = config.get('output_prefix', 'recon_slice')\n",
    "\n",
    "    logging.info(f\"Saving reconstructed volume to: {output_directory}\")\n",
    "    data_io.save_tiff_stack(reconstructed_volume, output_directory, file_prefix=output_prefix)\n",
    "    logging.info(\"Volume saving complete.\")\n",
    "else:\n",
    "    logging.warning(\"Skipping result saving as reconstruction was not successful or not performed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e554b",
   "metadata": {},
   "source": [
    "--- Pipeline End ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
